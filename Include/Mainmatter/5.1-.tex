\chapter{Application of the Risk-Neutral Density for Option Pricing}\label{Ch.5}\chaptermark{Application 1}
This chapter will utilise methods described in the previous chapters to derive the risk-neutral density for call options on numerous underlying stocks. The risk-neutral density will be estimated by constructing a neural network which approximates the implied volatility function, whose derivatives will be estimated using finite difference. Afterwards, the risk-neutral density will be used to derive option prices to test the method's performance. Throughout the application, we assume that this risk-neutral density is stationary - it depends only on the time to maturity $\tau=T-t$. We assume this to not restrict ourselves to only working with one contract as the data hereof is limited.

The project's code, data, and results are available on GitHub: \url{https://github.com/loprtq/P8}.


\section*{Data}
As mentioned, we will be working with numerous underlying stocks, specifically: Apple Inc. (AAPL), Amazon Inc. (AMZN), Tesla Inc. (TSLA), and Alphabet Inc. Class C (GOOG). These stocks have been chosen because of their varying liquidity and vast option data. For these stocks we retrieve stock prices, option prices and the options' strike prices and time to maturities. Additionally, we set the risk-free interest rate to $r=0.0368$ and calculate the implied volatility according to \eqref{Eq:IV_formula} using the function "\lstinline{EuropeanOptionImpliedVolatility}". We will first give a detailed description of the application of the stock GOOG, followed by a shorter analysis of the other stocks.

Given that we want to work with an implied volatility surface for which all data has the same stock price, we filter the data with respect to the stock price. For GOOG, this stock price is $S_t\approx100$ meaning retrieved data with stock prices below and above this are removed. Likewise, as we saw in \autoref{Sec.Implied_Volatility}, the volatility smile is more distinct when close to maturity, why we remove time to maturities larger than $0.1$ years (approximately 36 days). This restriction is applied as the implied volatility after this point is approximately zero. 

Having filtered the data for GOOG we end up with approximately $11.000$ data points, where the strike price and time to maturity is distributed as seen in \autoref{Figures:Figures/Pictures/KDist.pdf} and \autoref{Figures:Figures/Pictures/TDist.pdf}, respectively. As is evident in \autoref{Figures:Figures/Pictures/KDist.pdf}, most strike prices are around ATM ($100$) with very few points far away. Furthermore, the time to maturity is fairly spread out over the time period with gaps, which could be due to weekends. Because of this, it is expected that when constructing a neural network for the implied volatility function that it, in general, will be more precise around ATM as a consequence of the distribution of the data. 

\dimgfig{Figures/Pictures/KDist.pdf}{Distribution of strike prices for GOOG after filtering the data.}{Figures/Pictures/TDist.pdf}{Distribution of time to maturity for GOOG after filtering the data.}

When working with the data, we choose to normalise the inputs in the neural network, that is strike price and time to maturity, as this equalises the scale of input features. Additionally, some of the algorithms used to create the neural network performs better when using normalised data, as it, for example, helps the gradients converge to the minimum faster, and makes the network perform better in general.

The reason why strike price and time to maturity are the input variables in the neural network is that these are the variables the implied volatility surface depends on, given a stock price, $S_t$. This can also be seen in \autoref{Figures:Figures/Pictures/Volatility Surface Scatter (GOOG).png}, which is the implied volatility data points for the filtered data where $S_t = 100$. 

\imgfig[0.5]{Figures/Pictures/Volatility Surface Scatter (GOOG).png}{Implied volatility of GOOG for $S_t=100$.}

In \autoref{Figures:Figures/Pictures/Volatility Surface Scatter (GOOG).png} we see that as we approach maturity, the volatility smile becomes more skewed, and that there are few points far from ATM ($100$). Another thing to note is that the points that are further from ATM have higher implied volatility, resembling the theory presented in \autoref{Sec.Implied_Volatility}. All of this can affect the neural network constructed in the following section, and will be further analysed in later sections. 


\section{Neural Network for Implied Volatility}\label{Sec.App:NN}
For the construction of the neural network the package "\lstinline{tensorflow}" is used alongside the "\lstinline{keras}" package. After sorting the data there are, as mentioned, approximately $11.000$ input-output pairs, which are split into three batches: training-, validation-, and test sets. We have opted to allocate 50\% for training, and 25\% each for validation and testing, as this is the most widely used allocation. The validation set is utilised to identify overfitting by contrasting the errors of the neural network on the training set with that of the validation set. If the training set's error is below that of the validation set, it may suggest the presence of overfitting. An example of such behavior can be seen in \autoref{Figures:Figures/Pictures/Application/Overfitting_loss_mae.pdf}. Contrarily, if the training set's error is far above that of the validation set, it may suggest the presence of underfitting.

\imgfig[1]{Figures/Pictures/Application/Overfitting_loss_mae.pdf}{MSE, MAE and MAPE on training- and validation set.}

In \autoref{Figures:Figures/Pictures/Application/Overfitting_loss_mae.pdf} it is evident that throughout all $20$ epochs the neural network performs better on the training set than on the validation set, where the error is measured by the mean squared error (MSE), mean absolute error (MAE), and mean absolute percentage error (MAPE). MSE is chosen as it penalises the larger errors more heavily and is also widely used as loss function in neural networks. MAE is chosen as it gives an indication of the absolute average error, while MAPE is chosen because it takes the error's magnitude into account, making the errors easier to interpret and compare.

Another characteristic about the MAE and MAPE on the validation set in \autoref{Figures:Figures/Pictures/Application/Overfitting_loss_mae.pdf} is that it has big jumps. These jumps are not desirable as it indicates that the neural network is not learning in a stable and consistent manner. A validation error curve with sudden jumps can also indicate that the neural network is overfitting to the training set. This can, for example, be adjusted by changing the learning rate.

When constructing the neural network there is not one correct neural network, but choosing one with the smallest MSE, MAE, and MAPE as well as with no overfitting is advisable. Though, one should also take into consideration the reduction in error compared to the increase in parameters. This is because, as the number of parameters increase so does the number of computations, and the computational time.

We start the construction of the neural network by choosing the number of layers followed by the number of nodes in each layer. For this, we have chosen the loss function as the MSE since this is widely used for function approximations. Furthermore, the identity function is chosen as the activation function in the output layer, whilst a fixed activation function is chosen for the hidden layers. In order to ensure a fair comparison of network performance, we tested neural networks with a variety of layers and nodes, all trained for a fixed number of $100$ epochs, a minibatch size of $128$, and ReLU as the activation function. Specifically, we evaluated networks with one, two, three, and four layers, each with a wide range of nodes. To minimise variability in the experimental setup, we kept the optimiser fixed as the Adam optimiser with a learning rate of $\rho = 0.001$ and decay rates $\alpha=0.999$ and $\alpha_f=0.9$. Additionally, we used fixed training-, validation-, and test sets, as well as a seed for each network to ensure reproducibility. Finally, we did not apply any dropout to any of the layers. All of these precautions were taken to ensure that any observed differences in performance between the neural networks were due to their architectural differences, rather than other factors such as differences in training procedures or data partitioning.

Our comparison revealed that increasing the number of layers led to a significant improvement in both the MSE, MAE, and MAPE, with the greatest improvements observed when transitioning from one to two layers, and from two to three layers. However, when adding a fourth layer, we did not observe a clear improvement in network performance. Moreover, we observed an indication of overfitting as the validation and test errors exceeded that of the training set when adding a fourth layer. A sample of the different networks tested can be seen in \autoref{Table:NN}, where "\# of nodes" is the number of nodes in each hidden layer. As these tests were difficult to compare, we chose to focus on neural networks where the number of nodes in each hidden layer were equivalent. However, we additionally tested some neural networks with a differing number of nodes in each layer, which will be elaborated further upon when choosing the number of nodes.

\begin{table}[H]
    \centering
    {\renewcommand{\arraystretch}{1.25}\begin{tabular}{c|cccc}
        \# of nodes &  1 layer & 2 layers & 3 layers & 4 layers\\\hline
        5   & 7.5432\% & 7.2432\% & 7.1389\% & 7.1069\% \\ \hline
        20  & 7.1745\% & 5.7547\% & 5.3227\% & 5.3038\% \\ \hline
        60  & 7.0909\% & 5.2964\% & 5.0457\% & 4.9905\% \\ \hline
        110 & 6.2565\% & 5.1217\% & 4.8618\% & 4.8618\% \\ \hline
        210 & 6.1858\% & 5.0628\% & 4.8540\% & 4.8495\%
    \end{tabular}}
    \caption{MAPE for different neural networks all trained with a fixed seed.}
    \label{Table:NN}
\end{table}

As a result, we concluded that a neural network with three hidden layers provided the optimal performance, and we therefore chose to use this architecture for our subsequent analyses. The preceding analysis was conducted by evaluating up to forty networks for each number of layers, in order to ensure that our conclusions were robust and not based on a limited sample size. 

After choosing the number of layers, we went on to choose the optimal number of nodes in the neural network. Here we tried fifty combinations of number of nodes from very small networks with two nodes in each layer up to $250$ nodes in each layer. Furthermore, we experimented with the shape of the network. That is, if it was better to start with a wide layer and end with a narrow layer, vise versa, or just have the same width throughout all of the layers. We concluded that the neural network with $110$ nodes in each of the three hidden layers was the neural network that performed best without any clear indication of overfitting. This performance can also be seen in the fourth column of \autoref{Table:NN}, where the MAPE decreases approximately $0.2\%$ going from $5$ to $20$, $20$ to $60$, and $60$ to $110$ nodes, but only $0.02\%$ going from $110$ to $210$ nodes. That means our network has the same architecture as \autoref{Figures:Figures/Neural_networks/3_layer_NN.pdf} in \autoref{Ch.3}, where $K_0=2$, $K_1 = K_2 = K_3 = 110$ and $K_4 = 1$.

Having chosen the architecture of the network we experimented with minibatch sizes, activation functions, and other optimisers such as RMSProp, by training multiple different neural networks varying these three things. Within this training we established that a minibatch size of $32$, the Adam optimiser, and the ReLU activation function performed the best. One thing to note regarding the choice of activation function is that the ReLU is not differentiable in $0$, whilst the sigmoid function is. Therefore, it could have been a more suitable choice as the derivatives of the neural network are required later. However, when using the sigmoid function we observed much larger errors and it not being able to represent the volatility smile in the data very well after training the neural network. Lastly, the optimal combination of dropout-, learning-, and decay rates needs to be determined. However, it is too time-consuming to test all possible combinations of dropout-, learning-, and decay rates manually. Hence, we used the function "\lstinline{tuning_run}" from the package "\lstinline{tfruns}" which runs a hyperparameter tuning experiment based on a given set of parameters to choose from. We chose multiple dropout rates for each hidden layer and multiple learning- and decay rates all seen in \autoref{Table:NN2}. We chose that the function should test 30\% of the possible combinations at random from which we investigated the best performing ones.

\begin{table}[H]
    \centering
    {\renewcommand{\arraystretch}{1.25}\begin{tabular}{c|c}
        dropout rate layer 1  & $\left\{0, 0.2, 0.5 \right\}$\\ \hline
        dropout rate layer 2  &  $\left\{0, 0.2, 0.5\right\}$\\ \hline
        dropout rate layer 3  &  $\left\{0, 0.2, 0.5\right\}$\\ \hline
        learning rate         &  $\left\{0.0001, 0.0005, 0.001\right\}$\\ \hline
        decay rate $\alpha$   &  $\left\{0.9, 0.95, 0.99\right\}$\\ \hline
        decay rate $\alpha_f$ &  $\left\{0.8, 0.85, 0.9\right\}$\\ 
    \end{tabular}}
    \caption{Possible dropout-, learning, and decay rates for "\lstinline{tuning_run}".}
    \label{Table:NN2}
\end{table}

Even though it does not seem as many possible choices, it is a total of 730 different combinations where the function then chooses 219 random combinations. These results can be found in \url{https://github.com/loprtq/P8/tree/main/Code/Data}. From this there is a clear indication that the neural network performs best when the decay rates are $\alpha_f = 0.8$, $\alpha = 0.9$ and the learning rate is $\rho = 0.0005$. When the other hyperparameters are varied, the errors in the validation set are the same or very close to each other. Hence, we try training these neural networks for a greater number of epochs both to determine how the performance changes when training the neural network more, and to determine if some or all of the networks have an indication of overfitting. The result is that the neural network that performs the best when the dropout rate in the first, second and third layer is $0$, $0$ and $0.2$, respectively. Hence, we have chosen a neural network to work with based on the "\lstinline{tuning_run}" function, and manually checking for overfitting. That is, a neural network with two input nodes, one output node and three hidden layers each with 110 nodes, a dropout rate of $0.2$ in the last hidden layer, and decay rates $\alpha = 0.9$ and $\alpha_f = 0.8$. Furthermore, the neural network uses the ReLU activation function, has a learning rate of $\rho = 0.0005$ and a minibatch size of $32$. Lastly, this neural network keeps improving without any indication of overfitting up till $1200$ epochs. We also tried using a learning rate as the exponential decay described in \autoref{Ch.3}, but saw no clear improvement, why we did not use it.

The MSE, MAE and MAPE for the training- and validation set during the training of the neural network can be seen in \autoref{Figures:Figures/Pictures/Application/errorplot_final.pdf}.
\imgfig[1]{Figures/Pictures/Application/errorplot_final.pdf}{MSE, MAE and MAPE on training- and validation set during training of the neural network.}

In \autoref{Figures:Figures/Pictures/Application/errorplot_final.pdf} we observe that there is no clear indication of overfitting in either the MAE, MSE or MAPE plot. Moreover, there is some variability in the MAE and MAPE of the validation set. However, these values are so small that they are considered inconsequential. 

To further asses the neural network we plot an implied volatility surface constructed by the chosen neural network alongside the test set seen in \autoref{Figures:Figures/Pictures/Application/vol_surfaceNN.pdf}. 

\imgfig[1]{Figures/Pictures/Application/vol_surfaceNN.pdf}{Implied volatility surface from the neural network compared with test set (orange points).}

\autoref{Table:NN3} reveals that the MSE, MAE, and MAPE of the neural network on the test set are relatively small. These small errors suggest that the neural network performs very well, which is also the case as seen in \autoref{Figures:Figures/Pictures/Application/vol_surfaceNN.pdf}, where it exhibits a volatility smile like the one observed in the data.

\begin{table}[H]
    \centering
    {\renewcommand{\arraystretch}{1.25}\begin{tabular}{c|c}
        MSE  &  0.0002\\ \hline
        MAE  &  0.0089\\ \hline
        MAPE &  2.1075\%\\ 
    \end{tabular}}
    \caption{MSE, MAE and MAPE for the test set.}
    \label{Table:NN3}
\end{table}

Furthermore, to illustrate the error we plot the real values of the implied volatility against the prediction of the neural network, in a so called prediction plot, which is seen in \autoref{Figures:Figures/Pictures/Application/lineplot.pdf}. For the neural network to perform perfectly, its predictions must follow a diagonal line, illustrated as the black line in \autoref{Figures:Figures/Pictures/Application/lineplot.pdf}. In \autoref{Figures:Figures/Pictures/Application/lineplot.pdf} we see that even though the predictions do not follow the black line exactly, it is close.

\imgfig[1]{Figures/Pictures/Application/lineplot.pdf}{Prediction plot of the neural network approximating the implied volatility surface. The black line is the theoretical best linear regression whereas the blue line is the achieved linear regression.}

Furthermore, we have fitted a linear regression to the predictions as a function of the real values, which is illustrated by the blue line in \autoref{Figures:Figures/Pictures/Application/lineplot.pdf}. The linear regression is constructed using the "\lstinline{lm}" function where its coefficients and $R^2$ values can be seen in \autoref{tab:summary_af_lm}.

\begin{table}[H]
    \centering
    \begin{tabular}{ccc}
    \hline\addlinespace[1ex]
        \multicolumn{3}{c}{Coefficients}\\
          & Estimate & Std. Error \\
         Intercept & $0.0428$ & $0.0017$ \\
         Real Value & $0.8956$ & $0.0040$ \\
    \addlinespace[1ex]\hline\addlinespace[1ex]
        \multicolumn{3}{c}{$R^2$: $0.9435$, $R^2$ Adjusted: $0.9435$}\\
    \addlinespace[1ex]\hline
    \end{tabular}
    \caption{Summary of the model "\lstinline{lm(Predictions ~ Real Value)}".}
    \label{tab:summary_af_lm}
\end{table}

As described before, the neural network would perform perfectly if the "Estimate" had the value $1$ for "Real Value", $0$ for "Intercept", and $R^2=1$. However, it is seen that both the "Estimate" for "Real Value", and the $R^2$ are below one, more precisely $0.895618$ and $0.9435$, respectively. This indicates that the neural network performs fairly well, but there is still room for improvement. Further improvements can possibly be found, by again, using "\lstinline{tuning_run}" for an extended set of possible hyperparameters.

Note that, as mentioned, the neural network we have selected may not necessarily be the optimal one, as the process of choosing a suitable architecture is both complex and subjective. Additionally, due to time constraints, we were unable to experiment with an extensive amount of neural networks, and approaches. Hence, we acknowledge that our chosen network may have limitations, meaning there might exist a better neural network.

Using the neural network we then determine the derivatives of the implied volatility function with respect to the strike price once and twice. This is done as described in \autoref{Ch.4} by numerically differentiating the implied volatility function using finite difference. When applying the finite difference method on the implied volatility we started of by using the first and second order central difference method for calculating the derivatives. As the second order central difference did not perform very well we opted to try using the first order central difference, in \eqref{eq:central_diff}, twice in which case we obtained smoother results. This lead us to choose this methodology over the second order central difference method. Recall that the ReLU function is not differentiable, why we chose to approximate the derivatives numerically.

\input{Include/Mainmatter/5.2-}