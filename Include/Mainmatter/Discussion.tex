\chapter{Discussion}\label{Ch.Discussion}
% Data, likviditeter
A point which was not touched upon in the application was how the stocks were selected, as the purpose of the project is to show how numerical methods, such as neural networks, perform on real data. Thus, this real data had to have different attributes, such as differing liquidity, and data availability after the filtration. An important thing to note about the data is, however, that it is freely available, hence it not being as high quality as something you might pay for. For example, the retrieved data did not include the implied volatility, hence it having to be calculated. Furthermore, it is not possible to get option data for option further back in time. Hence, since we have retrieved data at different times for each stock, the analysis of the different options is performed for stocks with different maturities. However, the analysis should give a good indication on the performance of the methods used on different stock. Nevertheless, there is also a possibility that the methods would have performed better given higher quality data. 

% NN kan skabe arbitrage priser
A general problem of using deep learning methods in finance, and especially option pricing, is that neural networks might predict prices which are not arbitrage-free. This means that the prices are not to be trusted as these will not occur on the market. Thus, in general one should be careful when using neural networks for pricing in finance as the prices are not necessarily arbitrage-free. This is both a problem when approximating the implied volatility function as well as approximating the option pricing function. As already mentioned, this problem might become less of an issue if one had vast amounts of high quality data available, as the neural networks would thus have a better training basis. This is not to say the neural network is to be trusted more than a neural network trained on lower quality data, just that the results might be better. 

%% More data
If we wanted more data for training the neural network, we could perhaps have transformed the strike price axis of the numerous plots into moneyness or perhaps log-moneyness. Given that the strike price and stock price was available for all of the data, this could have lead to the size of the data sets being larger. Another reason to use moneyness (or log-moneyness) is because the neural networks constructed in this project requires the price of the underlying stock to be within a level. For GOOG this is approximately $100$, meaning that the network is only trained on data where ATM is at this value, and thus, for the project's networks to be effective the current stock price should be $100$. However, as just mentioned, if we were to only look at moneyness, this problem would disappear, as the current stock price would only be relevant to determine ATM. Thus, given that the networks were trained on moneyness it would be trained on more data. This has not been tested, but could be an interesting aspect to examine given additional time. However, no matter which network one chooses, there is the challenge of predicting stock prices. 

% Sv√¶rt at bestemme optimalt NN
As already mentioned numerous times throughout \autoref{Ch.3}, the process of choosing the architecture of a neural network for one specific problem can be time-consuming and subjective. There are so many parameters, functions and optimisers to choose from that choosing the absolute best network is almost impossible, even though functions such as \lstinline{tuning_run} exist. However, functions like this does give an impression of which parameters yield a better result than others, and hence the best network given some parameters to choose from. This makes a very time-consuming task a bit easier. The downside of this is that even though \lstinline{tuning_run} gives the best neural network with respect to the errors, one still has to go through many of the "best" networks to check for overfitting. Another very important thing to note regarding this problem of determining the best possible neural network, is whether or not the specific problem requires accuracy, speed, complexity, or a balance of these. Thus, in general, it is a very difficult and subjective process, hence the results depend a lot on the analysis and tuning process. Additionally, as the neural network becomes more complex and the dimension of the data set grows, the closer to a black-box model the neural networks become. This means that as the complexity and the dimension of the data set grows, the harder it is to interpret the models underlying operations. Building upon the topic of choosing the best neural network, we chose a neural network with three layers, $110$ nodes in each layer, and only $0.2$ as dropout rate in the last hidden layer. This gives us a total of $24.861$ parameters, which for all of the stocks means that there are more parameters than input-output pairs in the training set. This could easily cause the neural network to overfit to the training set in the training process. However, as seen in for example \autoref{Figures:Figures/Pictures/Application/errorplot_final.pdf} there is no clear indication of overfitting since the network performs better or equally good on the validation set compared to the training set. Although, there is some variation in the error in \autoref{Figures:Figures/Pictures/Application/errorplot_final.pdf} of the validation set, which could indicate some overfitting or at least that the network is not learning in a stable manner. However, as seen in \autoref{Table:NN3}, the MSE, MAE, and MAPE on the test set indicates that the chosen neural network performs very well on new data and hence the variation in the validation curve does not seem to matter much. Considering the potential for overfitting in the neural network, we tried fitting a smaller neural network on the data for GOOG from \autoref{Sec.App:NN} with three layers with $60$ nodes in each hidden layer. We observed that the MSE, MAE and MAPE on the test set became slightly worse but that the error of the option prices of the $100$ prices decreased to $123.44\%$ compared with the approximately $300\%$ when using the network in \autoref{Sec.App:NN}. This indicates that there might have been some overfitting in the network, even though there was no clear indication when comparing the performance on the training set with both the validation- and test set. This can potentially affect the derivatives of the implied volatility which affects the calculated option prices. A point of further investigation could thus be investigating other neural networks to test their comparative performance for the problem of predicting option prices using the risk-neutral density. Another advantage of using a smaller network is that both the training- and predictions process are significantly faster. To recapitulate, even though our large network provides good results when predicting the implied volatility a smaller network could be preferred because of multiple advantage. Therefore, based on the results, it could be argued that the neural network should have been chosen based on the performance when pricing the options, instead of predicting the implied volatility. While it is expected that these two factors should exhibit a consistent relationship, the above analysis demonstrates that this is not always the case.

% NN for optionspriser
In \autoref{Ch.5} and \autoref{Ch:app2} neural networks for the implied volatility function as well as the option pricing function were constructed. As the neural network for implied volatility does not output option prices, these have to be calculated using the risk-neutral density method, which requires the need for estimating derivatives and calculating integrals. These extra steps compared to the neural networks which outputs the option prices directly might be the reason why its performance is not as good. This is despite the fact that the neural network seems to be estimating the implied volatility surface fairly well. Thus, it could be due to the derivatives of the implied volatility being rough. Even though, the project focuses on numerical methods for almost all parts of this project, it has some drawbacks. A general drawback of these methods is that it is only approximations, and there is no guaranteed convergence. An advantage of numerical methods is, however, that it allows solutions to problems which do not have an analytic solution. A problem which might occur when working with multiple numerical methods is that if the methods have to be applied to the results of another numerical method. In this scenario, the error of the first numerical method is passed on to the second numerical method. This could result in the error increasing as more numerical methods are applied, however the errors could also be cancelling each other out. Thus, the approximation error of the implied volatility derivatives could come from numerous sources of errors. An approach to possibly solve this problem with the derivatives could be to directly use the derivatives of the function for the neural network. However, this would require the use of another activation function since ReLU is not differentiable. The downside of using another activation function would be that it results in a worse neural network, since we saw that ReLU outperformed sigmoid.

% Assumption
The assumption that the risk-neutral density was stationary and thus only dependent on the time to maturity might have been a too restrictive assumption. This could, as mentioned, also explain that the implied volatility surface for AMZN exhibits two parallel surfaces, which can be seen in \autoref{Figures:Figures/Pictures/TAA/vol_AMZN.png}. When further analysing the data for AMZN and filtering it such that there is only one maturity time we observe that it only has one surface. As none of the other stocks exhibited two clear surfaces, the stationarity assumption might only have affected AMZN. This does not mean that the assumption did not have any effect on the implied volatility surfaces just that it was not as obvious for the other stocks as it might have been with AMZN. We also tried testing the theory for a single maturity time for GOOG, where the results of both the risk-neutral density method and the option pricing method performed much worse when conducting the analysis for one maturity time. Hence, there are both things that argues in favour of the stationarity restriction, whilst the analysis on AMZN contradicts this. 