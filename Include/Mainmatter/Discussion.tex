\chapter{Discussion}\label{Ch.Discussion}
% Data, likviditeter
A point which was not touched upon in the application was how the stocks were selected, as the purpose of the project is to show how methods, such as neural networks, perform on real data. Thus, this real data had to have different attributes, such as differing liquidity, and data availability after filtering. An important thing to note about the data is, however, that it is freely available, hence it not being as high quality as something you might pay for. For example, the retrieved data did not include the implied volatility, hence it having to be calculated. Furthermore, it is not possible to get option data for option further back in time. Hence, since we have retrieved data at different times for each stock, the analysis of the different options is performed for stocks with different maturities. However, the analysis should give a good indication on the performance of the methods used on different stock. Nevertheless, there is also a possibility that the methods would have performed better given higher quality data. 

% NN kan skabe arbitrage priser
A general problem of using deep learning methods in finance, and especially pricing, is that the neural network might create prices which are not arbitrage-free. This means that the prices are not to be trusted as these will not occur on the market. This is both a problem when estimating the implied volatility as well as if one were to model option prices directly. As already mentioned, this problem might become less of an issue if one had vast amounts of high quality data available, as the neural networks would thus have a better foundation. This is not to say the neural network is to be trusted more, just that the results might be better.

% Sammenligning af data og likviditeter
%As seen in \autoref{Ch.5}, the four different stocks all performed very differently in the different metrics. This could be because the stocks are very different.

%% More data
Given that more data was required to create the neural network, one could perhaps have transformed the strike price axis of the numerous plots into moneyness or perhaps log-moneyness. Given that the strike price and stock price was available for all of the data, this could have lead to the size of the data sets being larger. Another reason to use moneyness (or log-moneyness) is because the neural networks created in this project requires the stock price of the option to be within a level. For GOOG this is $100$, meaning that the network is only trained on data where the ATM is at this value, and thus, for the project's networks to be effective the current price should be $100$. However, as just mentioned if we were to only look at moneyness, this problem would disappear, as the current stock price would only be relevant to determine ATM. Thus, given that the networks were trained on moneyness it would be trained on more data, and therefore not be as reliant on what exact stock price is but its moneyness. This has not be tested but could be an interesting aspect to examine given the time. However, no matter if one uses the network we have or one using moneyness, there will always occur a problem if one wants to predict the option prices. For this you should also know the stock price, and hence also use a method that predicts this fairly well. 

% Sv√¶rt at bestemme optimalt NN
As already mentioned numerous times throughout \autoref{Ch.3}, the process of choosing the architecture of a neural network for one specific problem can be time consuming and very subjective. There are so many parameters, functions and optimisers to choose from that choosing the absolute best network is almost impossible, even though functions such as \lstinline{tuning_run} exist. However, functions like this does give an impression on which parameters yield a better result than others, and hence the best network given some parameters to choose from. This makes a very time consuming task a bit easier. The down side of this is that even though \lstinline{tuning_run} gives the best neural network with respect to the errors, one still has to go through many of the "best" networks to check for overfitting. Another very important thing to note regarding this problem of determining the best possible neural network, is whether or not the specific problem requires accuracy, speed, convenience or balance of these. Thus, in general, it is a very difficult and somewhat subjective process, hence the results depend a lot on the analysis and order of the networks given different parameters. Additionally, as the neural network becomes more complex and the size of the data set grows, the closer to a black-box model, the neural networks become. This means that as the complexity and data set grows, the harder it is to interpret the models underlying operations. Building upon the topic of choosing the best neural network, we chose a network with three layers, $110$ and only $20\%$ dropout rate in the last hidden layer. This gives us a total of $24.861$ parameters, which for GOOG means that there are more parameters than data points in the training set. This could easily cause the neural network to overfit to the training set in the training process. However, as seen in \autoref{Figures:Figures/Pictures/Application/Overfitting_loss_mae.pdf} there is no clear indication of overfitting since the network performs better or equally good on the validation set compared to the training set. Although, there are some variation in the error in \autoref{Figures:Figures/Pictures/Application/Overfitting_loss_mae.pdf} of the validation set which also could indicate some overfitting or at least that the network is not learning in a stable manner. However, as seen in \autoref{Table:NN3}, the MSE, MAE, and MAPE of the test set indicates that the network which has been chosen performs very well on new data and hence the variation in the validation curve does not seem to matter much. Considering the potential for overfitting in the network we tried fitting a small neural network on the data for GOOG from \autoref{Sec.App:NN} with three layers of $20$ nodes in each. We saw that the MSE, MAE and MAPE on the test set became a little worse but that the error of the option prices of the 200 ATM prices decreased to $150.76\%$ compared with the approximately $300\%$ using the network in \autoref{Sec.App:NN}. This indicates that there have been some overfitting in the network, even though there were no clear indication when comparing the performance on the training set with both the validation and test set. This can potentially affect the derivatives of the implied volatility which affects the calculated prices. Hence a potential further research could additionally investigate other structures of the neural network, since it seems that the one we have chosen performs better than the small network when looking at the predictions of the implied volatility but is outperformed when predicting option prices using the risk-neutral density. Another advantage by using a smaller network is that both the training process and making predictions is significantly faster. To recapitulate, even though our large network provides good results when predicting the implied volatility a smaller network could be preferred because of multiple advantage. However, the network we have used also seem to perform well.

% NN for optionspriser
In \autoref{Ch.5} and \autoref{Ch:app2} neural networks for implied volatility as well as option prices were constructed. As the neural network for implied volatility does not output option prices, these have to be calculated using the risk-neutral density method, which requires the need for estimating derivatives and calculating integrals. These extra steps compared to the neural networks which output the option prices directly might be the reason why its performance is not as good. This is despite the fact that the neural network seems to be estimating the implied volatility surface fairly well. Thus, it could be due to the derivatives of the implied volatility being rough. Even though, the project focuses on numerical methods for almost all parts of this project, it has some drawbacks. A general drawback of these methods is that it is only approximations, and there is no guaranteed convergence. An advantage of numerical methods is, however, that it allows solutions to problems which do not have an analytic solution. A problem which might occur when working with multiple numerical methods is that if the methods have to be applied to the results of another numerical method. In this scenario, the error of the first numerical method is passed on to the second numerical method. This could result in the error increasing as more numerical methods are applied. This is not necessarily negative as the errors might not have been very large to begin with. Thus, the roughness of the implied volatility surfaces derivatives could come from numerous sources of error. A way to possible solve this problem with the derivatives could be to directly use the derivatives of the function from the neural network. However, this would require us to use another activation function since Relu is not differentiable. The down side of using this method will then be the use of a potentially worse neural network, since we saw that Relu outperformed both sigmoid and elu. 


