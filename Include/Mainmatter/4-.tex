\section{Finite Difference}\label{Ch.4}
As mentioned, the finite difference method will be used to determine the derivatives of the implied volatility function seen in \eqref{Eq:RND_Derivatives}. The principle of the finite difference method is to numerically approximate ordinary differential equations using difference quotients. Consider the Taylor series for a function $f\in C^{\infty}$
\begin{align}\label{eq:central_diff}
    f(a) &= \sum_{k=0}^\infty \frac{\partial^k f(t)}{\partial t^k}\frac{1}{k!}(a-t)^k, \nonumber %+ o\left((x-a)^{k}\right),
\intertext{then it can be rewritten in terms of the central difference operator, which yields}
    f(t+\Delta) - f(t-\Delta) &= \sum_{k=0}^\infty \frac{\partial^k f(t)}{\partial t^k}\frac{1}{k!}(\Delta)^k - \sum_{k=0}^\infty \frac{\partial^k f(t)}{\partial t^k}\frac{1}{k!}(-\Delta)^k \nonumber \\
    &= 2\frac{\partial f(t)}{\partial t}\Delta + 2\frac{\partial^3 f(t)}{\partial t^3}\frac{1}{3!}(\Delta)^3 + 2\frac{\partial^5 f(t)}{\partial t^5}\frac{1}{5!}(\Delta)^5 + \cdots. \nonumber
\intertext{Isolating for the first order derivative yields}
    \frac{\partial f}{\partial t} &= \frac{f(t+\Delta)-f(t-\Delta)}{2\Delta}+o(\Delta^2),
\end{align}
which is called the \emph{central difference} method. In \eqref{eq:central_diff}, the error term $o(\Delta^2)$ is the collection of the higher order derivatives. Likewise, if one were to use the forward- or backward difference operator the results would be
\begin{align*}
    \frac{\partial f}{\partial t}=\frac{f(t+\Delta)-f(t)}{\Delta}+o(\Delta)\quad\textrm{and}\quad\frac{\partial f}{\partial t}=\frac{f(t)-f(t-\Delta)}{\Delta}+o(\Delta),
\end{align*}
named accordingly. As this project also requires approximating second order derivatives, the second order central difference method is given as
\begin{align*}
    \frac{\partial^2 f}{\partial t^2} &= \frac{f(t+\Delta) - 2f(t) + f(t-\Delta)}{\Delta^2}+o(\Delta^2),
\end{align*}
where its derivation is similar to the first order central difference method.

In the equations above $o(\Delta)$, and $o(\Delta^2)$ decrease as $\Delta$ decreases, minimising the error of the derivative. Naturally, $o(\Delta^2)$ decreases the fastest for $|\Delta|<1$, why the central difference method is often used rather than the forward- and backward difference methods. To summarise, these methods, without their respective error terms, are good approximations for small $\Delta$. Hence, given a neural network, or another function approximating method, which produces a smooth enough function, finite difference can be easily implemented, especially for lower order derivatives.