\chapter{Neural networks}\label{Ch.3}
This chapter is based upon \citep[Ch. 10]{SL-ISLR}, \citep[Ch. 1]{NN}, and \citep[Ch. 2]{NNDL}, unless stated otherwise.

As described in \autoref{Sec.Calibration}, the implied volatility function has to be approximated numerically, with the focus in this project being neural networks. The name "neural network" and its construction is inspired by the human brain as it was designed to simulate its neurons and synapses. Due to its structure and flexibility, neural networks can be used for numerous purposes such as identifying handwritten numbers, forecasting time series and, approximating functions. Depending on the problem, the neural network can be constructed in different ways, where this project will be focusing on \emph{feed-forward} neural networks. The primary objective of these types of neural networks are to approximate an unknown function $f$, estimated through \emph{input-output pairs} $(x, y = f(x))$ defining a mapping $\hat{y} = \hat{f}(x)$. The name "feed-forward" refers to the fact that the neural network only feeds information in the direction from the input layer to the output layer. To provide a simple introduction to neural networks, a single-layer neural network, also called a \emph{perceptron}, is introduced in the following section. 


\section{Single-Layer Neural Network - Perceptron}
As mentioned, the perceptron is the simplest form of a neural network only consisting of an \emph{input}- and \emph{output layer}. In this section we will focus on a neural network with a one dimensional output. Hence, the input layer consists of $K_0$ \emph{nodes} each representing an input variable, $x_1, \ldots, x_{K_0}$, $0 < K_0 \in \N$, and the output layer consists of one node representing an output variable. Each node in the input layer has an \emph{edge} with an associated \emph{weight}, $w_1, \ldots, w_{K_0}$, to the node in the output layer. The output node takes the value of the linear combination $\sum_{i = 1}^{K_0} w_ix_i$ followed by applying an \emph{activation function} to predict the output variable. Activation functions often introduce non-linearity into the neural network, allowing it to model more complex relationships between input- and output variables. Two examples of activation functions are the sign function and the identity function. Besides the input nodes and weights, a \emph{bias} term, $b$, is sometimes needed to correct the linear combination for some invariant part of a model. In the computation of the prediction of the output variable this bias term is added to the linear combination before applying the activation function. For example, if the input variables, bias term and weights are given as above, $\hat{y}$ is the prediction of the output variable and the activation function is the sign function, then
\begin{align}\label{eq:sign_perceptron_output}
    \hat{y} &=  \operatorname{sign}\left(\sum_{i=1}^{K_0} w_ix_i + b\right).
\end{align}
 
A neural network can also be presented as a diagram where \autoref{Figures:Figures/Neural_networks/Preceptron-diag.pdf} is a example of a perceptron, with all of the aforementioned terms illustrated. This is the perceptron with $K_0$ nodes in the input layer and one node in the output layer. In \autoref{Figures:Figures/Neural_networks/Preceptron-diag.pdf} it is also seen that the bias term is incorporated in the diagram as a node with the value one and a weight $b$.
\imgfig[0.2]{Figures/Neural_networks/Preceptron-diag.pdf}{Visualisation of a perceptron with $K_0$ input nodes and one output node.}

Even though, as seen in \autoref{Figures:Figures/Neural_networks/Preceptron-diag.pdf}, and described above, the perceptron contains two layers (input and output), it is called a single-layer neural network. The reason for this is that all computations are performed in the output layer, meaning there is only one \emph{computational layer}. If there are multiple computational layers it is called a \emph{multi-layer neural network}, which will introduced in the following section. Additionally, the theory presented in the following section is a generalisation of that of the perceptron, and can therefore also be used for the perceptron. 


\section{Multi-Layer Neural Network}
As mentioned, multi-layer neural networks contain multiple computational layers, with the layers between the input- and output layer being called \emph{hidden layers}. These hidden layers, consists of nodes each with an edge from each node in the previous layer, and an edge to each node in the following layer, where each of edges have an associated weight. To visually aid this description, a diagram of a multi-layered neural network with $K_0$ input nodes, three hidden layers with respectively $K_1$, $K_2$ and $K_3$ nodes, and a output layer with $K_4$ output nodes is presented in \autoref{Figures:Figures/Neural_networks/3_layer_NN.pdf}, where $0<K_j \in \N$, $j = 0,\ldots, 4$.

\imgfig[0.6]{Figures/Neural_networks/3_layer_NN.pdf}{Visualisation a multi-layered neural network.}

In \autoref{Figures:Figures/Neural_networks/3_layer_NN.pdf} it is observed that for each computational layer there is an additional node representing a bias term, with connecting edges to each node in the computational layer.

Now that there are more computational layers than in the perceptron, additional calculations have to be performed to compute the values of the output layer. At each hidden layer similar calculations to the one in \eqref{eq:sign_perceptron_output} should be performed. Assume that the neural network contains $T$ computational layers, has $K_0$ nodes in the input layer with inputs $x_1,\ldots, x_{K_0}$ and uses a activation function $g_j$ in the $j$'th computational layer. Then values of the nodes in the first hidden layer are computed as
\begin{align*}
    A^{(1)}_k = g_1\left(b_{k}^{(1)} + \sum_{j=1}^{K_0} w_{k,j}^{(1)}x_j\right),
\end{align*}
where $k = 1,\ldots,K_1$ indicates which node in the first hidden layer and the superscript $(1)$ indicates that it is the first hidden layer. Furthermore, $b_{k}^{(1)}$ and $w_{k,j}^{(1)}$ are respectively the weight from the node with value $1$ and the $j$'th node in the input layer to the $k$'th node in the first hidden layer. When the values for all nodes in the first hidden layer have been computed it is possible to compute the values for the second hidden layer as
\begin{align*}
    A^{(2)}_l = g_2\left(b_{l}^{(2)} + \sum_{k=1}^{K_1} w_{l,k}^{(2)}A_k^{(1)}\right).
\end{align*}
Here a new weight for the bias term, $b_{l}^{(2)}$, is used together with the weights from the $k$'th node in the first hidden layer to the $l$'th node in the second hidden layer where $l = 1,\ldots, K_2$. Similarly, the remaining layers are computed recursively. Finally, the calculations for each node $\hat{y}_m$ in the output layer are given as
\begin{align*}
    \hat{y}_m = A^{(T)}_m = g_T\left(b_{m}^{(T)} + \sum_{j=1}^{K_{T-1}} w_{m,j}^{(T)}A_j^{(T-1)}\right).
\end{align*}
All the notation of these calculation, except the activation functions, is presented in \autoref{Figures:Figures/Neural_networks/3_layer_NN.pdf}, where $T = 4$. 

Given the previous explanatory theory, a formal definition of neural networks is now presented.
\begin{defn}[Neural Networks]\label{def:NN}
    Let $T \in \N$, and $(K_0, K_1, K_2, \ldots, K_T) \in\N^{T+1}$ be the number of computational layers and nodes in each layer, respectively. Then a neural network is a function from $\R^{K_0}$ to $\R^{K_T}$ defined as the composition
    \begin{align}\label{eq:NN}
        \bm x \in \R^{K_0} \mapsto g_T \circ z^{(T)} \circ g_{T-1} \circ z^{(T-1)} \circ \cdots \circ g_1 \circ z^{(1)}(\bm x).
    \end{align}
    Here $z^{(l)}: \R^{(K_{l-1})} \to \R^{(K_l)}$, for $l = 1, \ldots, T$ are affine functions represented by 
    \begin{align*}
        z^{(l)}(x) = W^{(l)} x + \bm{b}^{(l)},
    \end{align*}
    for weights $W^{(l)} \in \R^{K_l\times K_{l-1}}$ from layer $l-1$ to $l$ and bias term $\bm{b}^{(l)} \in \R^{K_l}$. Lastly, $g_l: \R \to \R$, for $l = 1, \ldots, T$ are called activation functions which are applied component-wise on $z^{(l)}$.
\end{defn} 

The weights and bias terms are the parameters of the neural network and will be denoted $\bm{\theta} \in \R^{N}$ where $N = \sum_{j = 0}^{T-1} K_{j+1}(1+K_{j})$ and the set of all possible parameters will be denoted $\Theta$. Furthermore, the function presented in \eqref{eq:NN} will be denoted $\hat{f}_{\bm{\theta}}$ and the set of all such neural networks for $\bm{\theta} \in \Theta$ is denoted $\mathcal{N\mkern-5mu N}_{K_0, K_T, T}(\Theta)$ or just $\mathcal{N\mkern-5mu N}_{K_0, K_T, T}$.

In any neural network, independent of the number of computational layers, the choice of activation function is important. Depended on what type of value the output should be there are different types of activation functions that can be used. For example, the identity function can be used if the predicted value of the output variable should be real. Furthermore, if one wants to get a probability of a class, functions such as the sigmoid can be used. Although the Rectified Linear Unit (ReLU) function is often used instead of the sigmoid function to ease some computations. Moreover, the activation function in the hidden layers and the output layer does not have to be the same. An example of this is using ReLU in the hidden layers and the identity function in the output layer, which is the case in the application of this project. For more information about the choice of activation functions, see \citep[p. 2-6]{NNDesign} and \citep[p. 11-13]{NNDL}.


\section{Backpropagation}
When the architecture of the neural network has been determined, the intention is then to choose the set of all weights, which includes the bias terms, denoted $\mathcal{W}$, such that the error between the prediction of the output variable, $\hat{\bm{y}}$ and the real value, $\bm{y}$, is minimised. This process is refereed to as training the network. To minimise this difference, the term \emph{loss function} is introduced, which is a function that describes this difference between the output of the network and real value. Minimising this function is accomplished by using \emph{training data}, which is chosen to be representative of the data that the network will be expected to handle in the future, containing input-output pairs $(\bm{x},\bm{y})$. In short, the training involves following the steps below, after assigning the weights some starting values.
\begin{enumerate}
    \item Forward Pass: Input data is fed to the neural network and computes output values. 
    \item Error Calculation: The difference between the output value and the real value is calculated using the chosen loss function.
    \item Backward Pass: The gradient of the loss function with respect to each weight in the neural network is calculated.
    \item Update Weights: The weights are updated using \emph{gradient descent}.
\end{enumerate}
These steps are then iterated for $N\in\N$ epochs, where an epoch refers to one complete iteration of the training set being fed into the neural network during the training process. These steps constitute what is called the backpropagation algorithm, which will be elaborated further upon in the following. 

The first of these steps is straightforward when the architecture of the neural network has been determined. When the first step has been completed, pairs of real and predicted values has been computed, $(\bm{y}, \hat{\bm{y}})$. In the second step one should first choose a loss function, that is, a measure of the difference between the real and predicted values. An example is the quadratic loss function, which for $n$ training input-output pairs is given as
\begin{align}\label{eq:loss_quadratic}
    L(\bm{x}, \mathcal{W}) &= \frac{1}{2n} \sum_{\bm x} \norm{\bm{y}(\bm{x}) - \hat{\bm{y}}(\bm{x})}^2, 
\end{align}
where $(\bm{y}(\bm x), \hat{\bm{y}}(\bm x))$ is the real and predicted values given a specific input $\bm x$ and weights, $\mathcal{W}$. The quadratic loss function will be used for the remainder of this section to explain backpropagation. \autoref{eq:loss_quadratic} can be written as
\begin{align}\label{eq:rewrite_loss_fct}
    L(\bm{x}, \mathcal{W}) &= \frac{1}{n} \sum_{\bm x} L_{\bm{x}}(\mathcal{W}),
\intertext{where}
    L_{\bm{x}}(\mathcal{W}) &= \frac{1}{2} \norm{\bm{y}(\bm{x}) - \hat{\bm{y}}(\bm{x})}^2 = \frac{1}{2} \sum_{j=1}^n (y_j(\bm{x}) - \hat{y}_j(\bm{x}))^2. \nonumber
\end{align}
Here, $L_{\bm{x}}$ is referred to as the individual loss function for each training input-output pair. Hence, the loss function is an average of the individual loss functions, which will be utilised in the next step of the training process. When the loss function has been chosen, it is then used to calculate the difference between the output- and the real value. In the third step the gradient of the individual loss function should be calculated, which will be used to minimise it, and hence minimise the loss function. This is accomplished by moving the weights in the opposite direction of the gradient, since this is the direction for which the loss function increases most rapidly. Hence, the new weights are given as
\begin{align}\label{eq:weight_update}
    \mathcal{W}_{m+1} = \mathcal{W}_{m} - \rho \frac{\partial L(\bm{x}, \mathcal{W}_m)}{\partial \mathcal{W}_m},
\end{align}
where from \eqref{eq:rewrite_loss_fct} 
\begin{align*}
    \frac{\partial L(\bm{x}, \mathcal{W})}{\partial \mathcal{W}} &=  \frac{1}{n} \sum_{\bm x} \frac{\partial L_{\bm{x}}(\mathcal{W})}{\partial \mathcal{W}}.
\end{align*}
Here the gradient descent is calculated given the set of weights $\mathcal{W}_m$, where $m$ refers to the $m$'th weight update. Furthermore, the \emph{learning rate} $\rho \in \R$ determines the step length, and can be a constant as well as a function. If chosen too small, the weights converge very slowly, but chosen too big one risks it not converging at all. A suitable learning rate for the specific purpose of the neural network is therefore important. An example of a non-constant learning rate is the exponential decay
\begin{align*}
    \rho_t = \rho_0 \exp{(-k \cdot t)},
\end{align*}
for an initial rate $\rho_0\geq0$, a constant $k \geq 0$, and as a function of which epoch, $t$, the neural network is in. This learning rate decreases for each epoch, and hence starts by taking large steps and then decreases the step length as one approaches a minimum. Other ways of choosing the learning rate can be read in \citep[p. 135-141]{NNDL}.

To minimise the loss function the gradient of the loss function with respect to all the weights should be known. First $\delta^{(i)}_{j}$ is introduced, which denotes the error term at the $i$'th layer and $j$'th node. At first $\delta^{(T)}_k$ has to be calculated, as this is used to calculate $\delta^{(T-1)}_j$, and so forth. Let the \emph{pre-activation} be given as
\begin{align*}
    z^{(i)}_{j} &= b_{j}^{(i)} + \sum_{k=1}^{K_{i-1}} w_{k,j}^{(i)}A_k^{(i-1)},
\end{align*}
then the $k$'th error term in the output layer is given as
\begin{align*}
    \delta^{(T)}_{k} =  \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial z^{(T)}_{k}} = \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial A_k^{(T)}}\frac{\partial A_k^{(T)}}{\partial z^{(T)}_{k}} = \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial A_k^{T}} g'(z^{(T)}_{k}).
\end{align*}
In general the $j$'th error term in the $i$'th hidden layer is given as
\begin{align*}
    \delta^{(i)}_{j} &= \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial z^{(i)}_{j}} = \sum_{k = 1}^{K_{i}} \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial z^{(i+1)}_{k}} \frac{\partial z^{(i+1)}_{k}}{\partial A^{(i)}_{j}}\frac{\partial A^{(i)}_{j}}{\partial z^{(i)}_{j}} =\sum_{k = 1}^{K_{i}} \delta^{(i+1)}_{k} \frac{\partial z^{(i+1)}_{k}}{\partial A^{(i)}_{j}}\frac{\partial A^{(i)}_{j}}{\partial z^{(i)}_{j}}\\
    &= \sum_{k = 1}^{K_{i}} \delta^{(i+1)}_{k} w_{k,j}^{i+1}g'\left(z_{j}^{(i)}\right).
\end{align*}
Hence, it is only possible to calculate the error terms in the $i$'th layer if the error terms in the $(i+1)$'th layer are known and so forth up until the output layer. Using this error term, it is possible to determine the gradient of the individual loss function with respect to all weights as
\begin{align*}
    \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial w_{k,j}^{(i)}} &= \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial z^{(i)}_{j}}\frac{\partial z^{(i)}_{j}}{\partial w_{k,j}^{(i)}} = \delta^{(i)}_{j} A_k^{(i-1)}\\
    \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial b_{j}^{(i)}} &= \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial z^{(i)}_{j}}\frac{\partial z^{(i)}_{j}}{\partial b_{j}^{(i)}} = \delta^{(i)}_{j}.
\end{align*}

In the third step, firstly all error terms should be calculated and secondly the gradient of the individual loss functions should be calculated. Lastly, one gets to step four where \eqref{eq:weight_update} is used to update the weights. When the weights have been updated from $\mathcal{W}_{m}$ to $\mathcal{W}_{m+1}$ the gradient with respect to these weights is calculated and used to update the weights to $\mathcal{W}_{m+2}$. This process can be repeated until a desired accuracy is reached. If the gradient is zero a possible global minimum has been reached.


\subsection{Stochastic Gradient descent}
As described, backpropagation relies on gradient descent to determine a minimum. Typically, it requires numerous epochs for gradient descent to reach a possible global minimum. Furthermore, the computations in each epoch quickly become very demanding depending on the network and the data set size. Therefore it is common to use \emph{stochastic gradient descent} (SGD) instead. The difference between the two methods is that SGD samples \emph{minibatches} of the training data, which are a random fractions of the training data. The gradient used to update the weights is then calculated based on one of these minibatches instead of the all the training data. After all the weights have been updated based on the first minibatch, a new gradient based on the next minibatch is then calculated and the weights are updated based on this. Hence, the weights are updated as in \eqref{eq:weight_update} although
\begin{align*}
    \frac{\partial L(\bm{x}, \mathcal{W})}{\partial \mathcal{W}} =  \frac{1}{s} \sum_{\bm x \in \mathbb{B}} \frac{\partial L_{\bm{x}}(\mathcal{W})}{\partial \mathcal{W}}
\end{align*}
for the set of $s\in\N$ input-output pairs in a minibatch $\mathbb{B} = \{(\bm{x}_1,\bm{y}_1), \ldots, (\bm{x}_s,\bm{y}_s)\}$. Using powers of 2 as minibatch sizes such as 32, 64, and 128, is a prevalent practice due to its tendency to maximise efficiency on a wide range of hardware architectures. Furthermore, when using SGD an epoch refers to the number of complete passes, through an equivalent of the full training set.


\subsubsection{Optimisers}
It is possible in both gradient descent and SGD to update the weights as in \eqref{eq:weight_update} for either each epoch or each minibatch, respectively. However, other methods can be used such as a root mean square propagation (RMSProp) algorithm or the adaptive moment estimation (Adam) algorithm. Even though both optimers can be used in both gradient descent and SGD, they are both designed for SGD and are hence more effective when using this method.

RMSProp is an example of a minibatch learning method developed to improve the speed, and stability of the convergence of the loss function. It does so by updating the weights after each minibatch by using the exponentially averaged aggregated squared gradient, which is given as
\begin{align}\label{Eq:RMSProp_updateA}
    \gamma_{m+1} = \alpha \gamma_m + \left(1-\alpha\right)\left(\frac{\partial L(\bm{x}, \mathcal{W}_m)}{\partial \mathcal{W}_m}\right)^2,\quad \gamma_0=0,
\end{align}
where $\alpha\in(0,1)$ is a \emph{decay parameter} and $m\in\N$ refers to the $m$'th minibatch in an epoch. The weight $\mathcal{W}_m$ for $m=0$ is the last weight obtained in the preceding epoch. The decay parameter can be chosen such that newer squared gradient have a more significant effect on the aggregation, than previous gradients. This is due to the squared gradient that occurred $t$ updates ago being weighted by $\alpha^{t}(1-\alpha)$. The weight $\mathcal{W}_m$ is then updated using $\gamma_m$ such that
\begin{align}\label{Eq:RMSProp_updatew}
    \mathcal{W}_{m+1} = \mathcal{W}_{m} - \frac{\rho}{\sqrt{\gamma_m}} \frac{\partial L(\bm{x}, \mathcal{W}_m)}{\partial \mathcal{W}_m},
\end{align}
where $\rho\in\R$ is the learning rate. In \eqref{Eq:RMSProp_updatew} the learning rate and the gradient is divided by the square-root of $\gamma_i$ to "signal-to-noise" normalise it. The Adam algorithm works in much the same manner as RMSProp, with the key difference between the two being that Adam exponentially smoothens the first order gradient as well. This means that even though both algorithms update their aggregated squared gradients in the same manner, the method they use to update the weights differ. RMSProp updates its weights according to \eqref{Eq:RMSProp_updatew}, whilst Adam exponentially smooths the first order gradient, meaning that the first order gradient $F_i$ is updated as
\begin{align}\label{Eq:Adam_updateF}
    F_{m+1} = \alpha_fF_m + \left(1-\alpha_f\right)\left(\frac{\partial L(\bm{x}, \mathcal{W}_m)}{\partial \mathcal{W}_m}\right),\quad F_0=0
\end{align}
where $\alpha_f\in(0,1)$ is another decay parameter than the one used in \eqref{Eq:RMSProp_updateA}. The purpose of this decay parameter is the same as the one in \eqref{Eq:RMSProp_updateA}, as it can be chosen such that the newer gradients have a larger effect on the aggregated gradient. The default suggested learning- and decay rates are $\rho=0.001$, $\alpha=0.999$, and $\alpha_f=0.9$, \citep[p. 141]{NN}. Thus, the default suggested decay rates are chosen such that the historical gradients have a more significant effect. The gradient $F_m$ is then used to update each weight $\mathcal{W}_{m}$ as
\begin{align}\label{Eq:Adam_updatew}
    \mathcal{W}_{m+1} = \mathcal{W}_{m} - \frac{\rho}{\sqrt{\gamma_m}}F_m.
\end{align} 
Both RMSProp and Adam are, as mentioned, used because they usually improve the speed and stability of the convergence of the loss function. The two optimisers accelerate the convergence by using the aggregated gradients to update the weights. This allows the optimisers to build up speed in directions where the gradient is consistently pointing, which can help the optimisers traverse flatter regions of the optimisation landscape more quickly, and avoid getting stuck in local optima. Furthermore, the use of exponential smoothing of the past gradients smoothens the updates of the weights, which can help reduce the oscillations in the optimisation process and lead to more stable convergence.


\section{Overfitting}
As should be evident by now, neural networks are models that can learn intricate relationships between inputs and outputs. However, if the neural network contains a large amount of computational layers and nodes, and the amount of training data is limited, this can result in modelling noise, which only exists in the training data. This phenomenon, known as overfitting, has prompted the development of various methods to mitigate it. One such method is \emph{dropout}, in which some nodes in the layers are temporarily excluded from the network, together with all connecting edges, and hence the associated weights, from and to these nodes, creating thinned neural networks. As a consequence, the excluded nodes do not contribute to the forward- or backward pass in the backpropagation algorithm of the network. 

A simple case of using dropout is that each node is preserved in the neural network with a probability $p$ independent of the other nodes. In several problems it has been concluded that a probability at $0.5$ is the optimal, but it can vary depending on the problem, and can be chosen based on the validation of the neural network. In the backpropagation algorithm the inputs get passed through the thinned neural networks and the gradient of the loss function is calculated, where the gradient of the loss function with respect to the dropped out weights is set to zero. This randomness in the network forces it to learn more robust features and reduces the reliance of the network on any one particular node or set of nodes, which helps prevent overfitting. Even though this helps prevent overfitting, the amount of thinned networks pose a problem when testing, since a neural network containing $n$ nodes has $2^n$ different possible thinned networks when using dropout. However, a straightforward approximate averaging technique can be effective, which involves using a single neural network without dropout. The weights are instead a scaled version of the original weights, with the outgoing weights of any node multiplied by the probability $p$ with which it was kept during training. Hence, the value of the $i$'th node in the $I$'th layer is given as
\begin{align*}
    A^{(I)}_i &= g\left(b_{i}^{(2)} + \sum_{k=1}^{K_{I-1}} w_{i,k}^{(2)}r_{k}^{(I-1)}A_{k}^{(I-1)}\right),
\end{align*}
where $r_{k}^{(I-1)} \thicksim \operatorname{Bernoulli}(p)$ and is independent from all $r_{j}^{(I-1)}$ where $j\in \{1,\ldots, k-1,k+1, \ldots, K_{I-1}\}$. To summarise, using the dropout method in this way results in a lower generalisation error on a broad range of classification problems \citep{Dropout}.

As mentioned, other methods aside from dropout exist, however, as the application will only use dropout these will not be introduced.

\section{Function Approximators}\label{sec:UAT}
This section is based upon \citep[p. 16-17]{Art}, \citep{Barron} and \citep{REOS}.

At the beginning of the chapter, it was stated that the purpose of neural networks is to approximate a function for the implied volatility. In this section \emph{the universal approximation theorem} will be introduced, which asserts that specific functions, referred to as target functions, can be approximated by neural networks defined as in \autoref{def:NN}. 

In this section, it will be assumed that the activation function in the hidden layers is the function $g$, that is, $g_1 = g_2 = \cdots = g_{T-1} = g$, whilst the activation function in the output layer is the identity function. The identity function is often used in the output layer for function approximation tasks because it allows the network to directly output the predicted value of the target function without any transformation or scaling. This can simplify the training process and make it easier to interpret the output of the network. Additionally, it will be assumed that the output layer has dimension one, which means that the target function is one dimensional. All these assumptions match with the ones in the application of this project. The set of neural networks with the assumptions given above will be denoted $\mathcal{N\mkern-5mu N}^g_{K_0, 1, T}$ using the notation from \autoref{def:NN}. 

Note that the universal approximation theorem can be extended to countless versions, some without the assumptions given above. The universal approximation theorem, \autoref{Thm:UAT}, is chosen for its applicability to this project. 
\begin{thm}[The Universal Approximation Theorem] \label{Thm:UAT}
    Let the set of neural networks $\mathcal{N\mkern-5mu N}^g_{K_0, 1, T}$ be given as in \autoref{def:NN} with the assumptions given above. If the activation function $g$ is non-constant and continuous and the target function $f:\R^n \to \R$, $n\in \N$ is in $L^2(\mu)$, then $\mathcal{N\mkern-5mu N}^g_{K_0, 1, T}$ is dense in $L^2(\mu)$ for all finite measures $\mu$. That is for every $\varepsilon > 0$ there exists a function $\hat{f}_{\theta} \in \mathcal{N\mkern-5mu N}^g_{K_0, 1, T}$ such that
    \begin{align*}
        \norm{f - \hat{f}_{\theta}}_2 < \varepsilon, 
    \end{align*}
    where $\norm{\cdot}_2$ is the $L^2$-norm.
\end{thm}
\begin{proof}
    The proof of this theorem is outside the scope of this project, but can be found in \citep{Hornik}.
\end{proof}

In short, \autoref{Thm:UAT} states that it is possible to approximate any one dimensional function in $L^2$ arbitrarily close using a function in $\mathcal{N\mkern-5mu N}^g_{K_0, 1, T}$. As mentioned, multiple different versions of this theorem exist. An example of another version of this theorem is one wherein specific activation functions, or network depth and width, are required. Furthermore, another one states that a neural network with a single hidden layer can approximate a function of class $C^n$, and all it derivatives up to order $n$ if the activation function is of class $C^n$. Hence, if one wants to insure that the $d$'th order derivative is well-defined one should choose an activation function $g \in C^n$, $n \geq d$. This also implies that even though the ReLU is often chosen instead of the sigmoid function as the activation function, one should be careful using the ReLU when approximating derivatives as it not differentiable. Conversely, the sigmoid function can, as mentioned, be used as activation function when approximating both a function and its derivatives.

Another advantage of the sigmoid function is that \citep{Barron} provides results stating that the approximation error when using the sigmoid function is bounded by $C_f/\sqrt{n}$ where $C_f$ can be regarded as a constant and $n$ the number of nodes. This result thus indicates that as the number of nodes increase, the approximation error decreases. Even though this is only stated for the sigmoid function, \citep[s. 117]{Barron} states that similar results can be obtained for multiple other activation functions.

Despite the potential effectiveness of the single-layer neural networks, their practicality is limited due to their exponential increase in dimension. Moreover, their high susceptibility to overfitting adds to their impracticality. When designing a neural network, a fundamental question arises regarding the optimal trade-off between width and depth, where width and depth represents number of nodes in the layers and number of layers, respectively. Should one choose a narrow and deep network with multiple layers but fewer nodes per layer, or a shallow and wide network? Empirical evidence suggest that depth plays a crucial role in the performance of a neural network. Deep architectures tend to result in more complex models that are challenging to achieve with shallow networks, \citep{REOS}. In fact, Theorem 1 in \citep{REOS} states that the approximation of a function in a two layer neural network will require exponentially many nodes compared to a three layered neural network to gain the same accuracy. Hence, a narrow and deep neural network is often preferred.