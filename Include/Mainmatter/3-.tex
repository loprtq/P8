\chapter{Neural networks}\label{Ch.3}
This chapter is based upon \citep[Ch. 10]{SL-ISLR}, \citep[Ch. 1]{NN} and \citep[Ch. 2]{NNDL}, unless stated otherwise.

%\begin{itemize}
%    \item $x_1,...x_n$ expanatory variables
%    \item $b_1,...,b_n$ bias 
%    \item $g$, activation function
%    \item $w_1,...,w_n$ weights
%    \item $\hat{y}$, prediction of the denpendent variable
%    \item Nodes and layers
%    \item Epoch: times iterated through the training data
%\end{itemize}

There are several different methods of calibrating the implied volatility described in \autoref{Sec.Calibration}. One method that can be used is \emph{neural networks}, that can approximate the function for the implied volatility which follows from the universal approximation theorem that will be presented in \autoref{sec:UAT}. The name "neural network" comes from it being a method of simulating the mechanism of the human nervous system. The neural network takes an input of some explanatory variables and constructs a function which output is a prediction of a dependent variable or variables. Neural networks can be used for multiple purposes such as identifying handwritten numbers, forecasting time series and, as mentioned, calibrating implied volatility. Depending on the problem neural networks can be constructed in different ways. In the following section, the simplest form of a neural network is presented which is a single layer neural network also called a \emph{perceptron}.


\section{Single Layer Neural Network - the perceptron} 
The perceptron is constructed by an \emph{input layer} and an \emph{output layer}, where the input layer contains a number of \emph{nodes} which each represent an input variable, $x_1, \ldots, x_n$. From each node in the input layer there is an edge with an associated \emph{weight}, $w_1, \ldots, w_n$, to the output nodes in the output layer. The output nodes take the value of the linear combination $\sum_{i = 1}^n w_ix_i$ followed by applying an \emph{activation function} to predict the dependent variable also called the output variable. This activation function can, for example, be the sign function or the identity function. Besides the input nodes and weights, a \emph{bias} term, $b$, is sometimes needed to correct the linear combination for some invariant part of a model. In the computation of the prediction of the output variable this bias term is added to the linear combination before applying the activation function. For example, if the input variables, bias term and weights are given as above, $\hat{y}$ is the prediction of the output variable and the activation function is the sign function, then
\begin{align}\label{eq:sign_perceptron_output}
    \hat{y} =  \operatorname{sign}\left(\sum_{i=1}^n w_ix_i + b\right).
\end{align}
 
A neural network can also be presented as a diagram where \autoref{Figures:Figures/Neural_networks/Preceptron-diag.pdf} is a example of a perceptron, where all the aforementioned terms are illustrated. This is the perceptron with $n$ nodes in the input layer and one node in the output layer. In \autoref{Figures:Figures/Neural_networks/Preceptron-diag.pdf} it is also seen that the bias term is incorporated in the diagram as a node with the value one and a weight, $b$.
\imgfig[0.2]{Figures/Neural_networks/Preceptron-diag.pdf}{Visualisation of a perceptron with $n$ input nodes and one output node.}

%evt ved brug af https://tikz.net/neural_networks/

Even though, as seen in \autoref{Figures:Figures/Neural_networks/Preceptron-diag.pdf} and described above, the perceptron contains two layers (input and output), it is called a single layer neural network. The reason for this is that all computations are performed in the output layer, meaning there is only one \emph{computational layer}. If there are more computational layers it is called a \emph{multilayer neural network}, which will now be described. The theory presented in the following section is a generalisation of that of the perceptron, and can therefore also be used for the perceptron. 


\section{The Multilayer Neural Network}
As just mentioned multilayer neural networks contains in addition to the input- and output layer further computational layers called \emph{hidden layers}. These additional computational layers are in between the input- and output layer and each layer contains a number of nodes. As for the perceptron the network only feeds information in the direction from the input layer to the output layer and is hence called a \emph{feed-forward} neural network. Furthermore, it is assumed that all nodes in one layer is connected to all nodes in the next layer with associated weights. A diagram of a multilayered neural network with $n$ input nodes, three hidden layers with respectively $K_1$, $K_2$ and $K_3$ nodes in, and a output layer with $K_4$ output nodes is presented in \autoref{Figures:Figures/Neural_networks/3_layer_NN.pdf}
\imgfig[0.6]{Figures/Neural_networks/3_layer_NN.pdf}{Visualisation a multilayered neural network.}

In \autoref{Figures:Figures/Neural_networks/3_layer_NN.pdf} it is seen that for each computational layer there is an additional node representing a bias term, with connecting edges to each node in the computational layer.

Now that there are more computational layers than in the perceptron, additional calculations have to be performed to compute the values of the output layer. At each hidden layer similar calculation to the one done in \eqref{eq:sign_perceptron_output} should be performed. If one assumes that the neural network contains $T$ computational layers, uses a activation function $g$ and has $n$ nodes in the input layer with inputs $x_1,\ldots, x_n$. Then the first hidden layer is computed as
\begin{align*}
    A^{(1)}_k = g\left(b_{k}^{(1)} + \sum_{j=1}^n w_{kj}^{(1)}x_j\right),
\end{align*}
where $k = 1,\ldots,K_1$ indicates which node in the first hidden layer is calculated and the superscript $(1)$ indicates that it is the first hidden layer. Furthermore, $b_{k}^{(1)}$ and $w_{kj}$ are respectively the weight from the node with value $1$ and the $j$'th node in the input layer to the $k$'th node in the first hidden layer. When the values for all nodes in the first hidden layer have been computed it is possible to compute the values for second hidden layer as
\begin{align*}
    A^{(2)}_l = g\left(b_{l}^{(2)} + \sum_{k=1}^{K_1} w_{lk}^{(2)}A_k^{(1)}\right).
\end{align*}
Here a new weight for the bias term, $b_{l}^{(2)}$, is used together with the weights from the $k$'th node in the first hidden layer to the $l$'th node in the second hidden layer where $l = 1,\ldots, K_2$. This can be done for each hidden layer up to and including the $(T-1)$'th hidden layer
\begin{align*}
    A^{(T-1)}_j = g\left(b_{j}^{(T-1)} + \sum_{i=1}^{K_{T-2}} w_{ji}^{(T-1)}A_i^{(T-2)}\right).
\end{align*}
Furthermore, it is possible to compute the calculations for each node $\hat{y}_m$ in the output layer as
\begin{align*}
    \hat{y}_m = g\left(b_{m}^{(T)} + \sum_{j=1}^{K_{T-1}} w_{mj}^{(T)}A_j^{(T-1)}\right).
\end{align*}
All the notation of these calculation is presented in \autoref{Figures:Figures/Neural_networks/3_layer_NN.pdf}, where $T = 4$. %The weights can also be noted as matrices for each layer, $\bm{W}_1, \bm{W}_2,\ldots \bm{W}_T, \bm{W}_{T+1}$. In $\bm{W}_1$ the $k,j$ entrance will represent $w_{kj}^{(1)}$. 

% As noted in the bottom of this diagram the weights can be noted as matrices for each layer, $\bm{W}_1, \bm{W}_2,\ldots \bm{W}_T, \bm{W}_{T+1}$. In $\bm{W}_1$ the $k,j$ entrance will represent $w_{kj}^{(1)}$. In the same way the nodes and the weights from the bias term in each layer can be written as vectors. Hence the above calculation can be written as
% \begin{align*}
%     \bm{A}^{(1)} &= g(\bm{b}^{(1)} + \bm{W}_1\bm{x})= g(\bm{z}^{(1)}) &&\text{(First hidden layer)}\\
%     \bm{A}^{(i)} &= g(\bm{b}^{(i)} + \bm{W}_i\bm{A}^{(i-1)}) = g(\bm{z}^{(i)})&&\text{(Second to last hidden layer)}\\
%     \bm{\hat{y}} &= g(\bm{b}^{(T+1)} + \bm{W}_{t+1}\bm{A}^{(T)})= g(\bm{z}^{(T+1)})&&\text{(Output layer)}
% \end{align*}
% where $g$ is applied to each entrance. Further $\bm{B}$ will contain all vectors for the weight for the bias term, $\bm{w}^k$, where this is the $k'$th row in the matrix. 

In any neural network, independent of the number of computational layers, the choice of activation function is important. Depended on what the type value the output should be there are different types of activation functions that can be used. For example the identity function can be used if the predicted value of the dependent variable should be real. Furthermore, if one wants to get a probability of a class, functions such as the sigmoid can be used. Though the Rectified Linear Unit (ReLU) function is often used instead of the sigmoid to ease some computations. Moreover, the activation function in the hidden layers and the output layer does not have to be the same. The ReLU function can be used in the hidden layers of a multilayered neural network, while the identity function is used in the output layer. 


\section{Backpropagation}
When the structure of the neural network has been build, the intention is then to choose the set of all weights, denoted $\mathcal{W}$, such that the error between the output, $\bm{\hat{y}}$ and the real value, $\bm{y}$, is minimised. This process is also refereed to as training the network. To minimise this difference, the term \emph{loss function} is introduced, which is a function that describes this difference between the output and real value. Minimising this function is done by using \emph{training data} which is data chosen to be representative of the data that the network will be expected to handle in the future containing input-output pairs $(\bm{x}_i,\bm{y}_i)$. In short, the training is done by the following steps after assigning the weights some starting values.
\begin{enumerate}
    \item Forward Pass: Input data is fed to the neural network and computes output values. 
    \item Error Calculation: The difference between the output and the real value is calculated using the chosen loss function.
    \item Backward Pass: The gradient of the loss function with respect to each weight in the neural network is calculated.
    \item Update Weights: The weights are updated using gradient descent.
\end{enumerate}
These steps are then repeated, where each time the training data is fed into the neural network is called an epoch. These steps constitute what is called the backpropagation algorithm, which will be elaborated further upon in the following. 

The first of these steps are straight forward when the structure of the neural network has been computed. When the first step has been completed, pairs of real and predicted values has been computed, $(\bm{y}_i, \bm{\hat{y}}_i)$. In the second step one should first choose a loss function, that is, a method to measure the difference between the real and predicted values. An example is the quadratic loss function, which for $n$ training input-output pairs is given as
\begin{align}\label{eq:loss_quadratic}
    L(\bm{x}, \mathcal{W}) &= \frac{1}{2n} \sum_{\bm x} \norm{\bm{y}(\bm{x}) - \bm{\hat{y}}(\bm{x})}^2, 
\end{align}
where $(\bm{y}_i(\bm x), \bm{\hat{y}}(\bm x))$ is the real and predicted values given a specific input $\bm x$ and weights, $\mathcal{W}$. The quadratic loss function will be used for the remainder of this section to explain backpropagation. \autoref{eq:loss_quadratic} can be written as
\begin{align}\label{eq:rewrite_loss_fct}
    L(\bm{x}, \mathcal{W}) &= \frac{1}{n} \sum_{\bm x} L_{\bm{x}}(\bm{x}, \mathcal{W}),
\intertext{where}
    L_{\bm{x}}(\bm{x}, \mathcal{W}) &= \frac{1}{2} \norm{\bm{y}(\bm{x}) - \bm{\hat{y}}(\bm{x})}^2 = \frac{1}{2} \sum_{j=1}^n (y_j(\bm{x}) - \hat{y}_j(\bm{x}))^2. \nonumber
\end{align}
Here, $L_{\bm{x}}$ is referred to as the individual loss function for each training input-output pair. Hence the loss function is a average of the individual loss functions. This can be used in the third step of the training process. Here the individual loss function is minimised using \emph{gradient descent}. In the third step the gradient of the individual loss function should be calculated, which will be used to minimise it and hence minimise the loss function. From \eqref{eq:rewrite_loss_fct} 
\begin{align*}
    \frac{\partial L(\bm{x}, \mathcal{W})}{\partial \mathcal{W}} &=  \frac{1}{n} \sum_{\bm x} \frac{\partial L_{\bm{x}}(\bm{x}, \mathcal{W})}{\partial \mathcal{W}}
\end{align*}
which can be used to compute new weights. This is done by moving the weights in the opposite direction of the gradient, since this is the way for which the loss function increases most rapidly. Hence, the new weights are given as
\begin{align}\label{eq:weight_update}
    \mathcal{W}_{m+1} = \mathcal{W}_{m} - \rho \frac{\partial L(\bm{x}, \mathcal{W}_m)}{\partial \mathcal{W}_m},
\end{align}
where the gradient descent is calculated given the set of weights $\mathcal{W}_m$. The \emph{learning rate}, $\rho$, determines the step length, and can both be a constant or a function. If chosen too small, the weights converge very slow, but chosen too big one risks it not converging at all. Hence, one should choose a suiting learning rate, which for example could be chosen as the exponential decay
\begin{align*}
    \rho_t = \rho_0 \exp{(-k \cdot t)}
\end{align*}
for a initial rate $\rho_0$, a constant $k$ and as a function of which epoch, $t$, the neural network is in. This learning rate decreases for each epoch, and hence starts by taking large steps and then decreases the step length as one approaches the minimum. Other ways of choosing the learning rate can be found in \citep[p. 135-141]{NNDL}. 

When the weights have been updated to $\mathcal{W}_{m+1}$ the gradient with respect to these weights is calculated and used to update the weights again to $\mathcal{W}_{m+2}$. If the gradient is zero a possible minimum has been reached. Therefore to minimise the loss function the gradient of the loss function with respect to all the weights should be known. Let
\begin{align*}
    z^{(i)}_{j} = b_{j}^{(i)} + \sum_{k=1}^{K_{i-1}} w_{kj}^{(i)}A_k^{(i-1)},
\end{align*}
then 
\begin{align*}
    \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial w_{kj}^{(i)}} &= \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial z^{(i)}_{j}}\frac{\partial z^{(i)}_{j}}{\partial w_{kj}^{(i)}} = \delta^{(i)}_{j} A_k^{(i-i)}\\
    \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial b_{j}^{(i)}} &= \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial z^{(i)}_{j}}\frac{\partial z^{(i)}_{j}}{\partial b_{j}^{(i)}} = \delta^{(i)}_{j}
\end{align*}
using the chain rule. Here $\delta^{(i)}_{j}$ is called the error term at level $i$ node $j$. At first $\delta^{(T)}_k$ has to be calculated, as this is used to calculate $\delta^{(T-1)}_j$ and so forth. The $k$'th error term in the output layer is given as
\begin{align*}
    \delta^{(T)}_{k} =  \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial z^{(T)}_{k}} = \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial A_k^{(T)}}\frac{\partial A_k^{(T)}}{\partial z^{(T)}_{k}} = \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial A_k^{T}} g'(z^{(T)}_{k}).
\end{align*}
In general the $j$'th error term in the $i$'th hidden layer is given as
\begin{align*}
    \delta^{(i)}_{j} &= \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial z^{(i)}_{j}} = \sum_{k = 1}^{K_{i}}  \frac{\partial L_{\bm{x}}(\mathcal{W}_m)}{\partial z^{(i+1)}_{k}} \frac{\partial z^{(i+1)}_{k}}{\partial A^{(i)}_{j}}\frac{\partial A^{(i)}_{j}}{\partial z^{(i)}_{j}} =\sum_{k = 1}^{K_{i}}  \delta^{(i+1)}_{k} \frac{\partial z^{(i+1)}_{k}}{\partial A^{(i)}_{j}}\frac{\partial A^{(i)}_{j}}{\partial z^{(i)}_{j}}\\
    &= \sum_{k = 1}^{K_{i}}  \delta^{(i+1)}_{k} w_{kj}^{i+1}g'\left(z_{j}^{(i)}\right).
\end{align*}
Hence it is only possible to calculate the error terms in the $i$'th layer if the error terms in the $(i+1)$'th layer are known. 

Hence, in the third step, firstly all error terms should be calculated and secondly the gradient of the individual loss functions can be calculated. Lastly, one gets to step four where \autoref{eq:weight_update} is used to update the weights. 


\subsection{Stochastic Gradient descent}
As described, backpropagation relies on gradient descent to determine minima. Typically, it requires numerous epochs for gradient descent to converge to a local minimum. Furthermore, the computations in each epoch quickly becomes very demanding depending on the network. Therefore it is common to use the \emph{stochastic gradient descent} (SGD) instead. The difference between the two methods is that SGD samples \emph{minibatches} of the training data, which are a random fractions of the training data. The gradient used to update the weights is then calculated from one of these minibatches instead of the all the training data. After all the weights have been updated with respect to the first minibatch, a new gradient with respect to the next minibatch is then calculated and the weights are updated based on this. Hence, the weights are updated as in \eqref{eq:weight_update} although
\begin{align*}
    \frac{\partial L(\bm{x}, \mathcal{W})}{\partial \mathcal{W}} =  \frac{1}{s} \sum_{\bm x \in \mathbb{B}} \frac{\partial L_{\bm{x}}(\bm{x}, \mathcal{W})}{\partial \mathcal{W}}
\end{align*}
for the set of $s$ input-output pairs in a minibatch $\mathbb{B} = \{(\bm{x}_1,\bm{y}_1), \ldots, (\bm{x}_s,\bm{y}_s)\}$. Using powers of 2 as mini-batch sizes such as 32, 64, 128, is a prevalent practice due to its tendency to maximise efficiency on a wide range of hardware architectures. Furthermore, when using SGD an epoch refers to the number of complete passes through an equivalent of the full training set.


\section{Overfitting}
As seen neural networks are models that can learn intricate relationships between inputs and outputs. However, if the neural network contains a large amount of computational layers and nodes, and the amount of training data is limited, this can result in modelling noise only existing in the training data. This phenomenon, known as overfitting, has prompted the development of various methods to mitigate it. One such method is \emph{dropout}, in which some nodes in the layers are temporarily excluded from the network, together with all edges, and hence the associated weights, from and to these nodes, creating thinned neural networks. As a consequence the excluded nodes do not contribute to the forward or backward pass in the backpropagation algorithm of the network. 

A simple case of using dropout is that each node is preserved in the neural network with a probability $p$ independent of the other nodes. In several problems it has been concluded that a probability at $0.5$ is the optimal, but it can vary depending on the problem and can be chosen based on the validation of the neural network. Then in the backpropagation algorithm the inputs goes through the thinned neural networks and the gradient of the loss function calculated where it is zero when with respect to the dropped out weights. This randomness in the network forces it to learn more robust features and reduces the reliance of the network on any one particular node or set of nodes, which helps to prevent overfitting. Even though this prevents overfitting, the amount of thinned networks pose a problem when testing since in a neural network containing $n$ nodes, there are $2^n$ different possible thinned networks when using dropout. However, a straightforward approximate averaging technique can be effective, which involves using a single neural network without dropout. The weights are instead a scaled version of the original weights, with the outgoing weights of any node multiplied by the probability $p$ with which it was kept during training. Hence if the value of the $i$'th node in the $I$'th layer is given as
\begin{align*}
    A^{(I)}_i = g\left(b_{i}^{(2)} + \sum_{k=1}^{K_{I-1}} w_{ik}^{(2)}r_{k}^{(I-1)}A_{k}^{(I-1)}\right).
\end{align*}
where $r_{k}^{(I-1)} \thicksim \operatorname{Bernoulli}(p)$ and is independent from all $r_{j}^{(I-1)}, \ j\in \{1,\ldots, j-1,j+1, \ldots, K_{I-1}\}$. It has been concluded that using the dropout method in this way results in a lower generalisation error on a broad range of classification problems \citep{Dropout}.

Dropout is not the only method of preventing overfitting such as L1 and L2 regularization\textbf{kilde til noget om dette}, but we will only be using the dropout method.


\section{Function Approximators}\label{sec:UAT}
This section is based upon \citep[p. 16-17]{Art}, \citep{Barron} and \citep{REOS}.

At the beginning of the chapter, it was stated that the neural network's purpose is to estimate a function for the implied volatility. In this section \emph{the universal approximation theorem} will be introduced, which asserts that specific functions, 
referred to as the target function, can be approximated by neural networks.
\begin{thm}[The Universal Approximation Theorem] \label{Thm:UAT}
    Let $\mathcal{NN}_{d_0, 1}^g$ denote the set of single layer neural networks with activation function $g: \R \to \R$, input dimension $d_0 \in \N$ and output dimension $1$. Further let $f: \R^{d_0} \to \R$ be the target function of class $C^n$. If the activation function is non-constant and $g \in C^n$, then $\mathcal{NN}_{d_0, 1}^g$ arbitrarily approximates $f$ and all its derivatives up to order $n$.
\end{thm} 
\begin{proof}
    Omitted.
\end{proof}

This means that single layer neural networks can arbitrarily approximate a continuous differentiable function and its derivatives if the activation function is chosen correctly. Hence, if one wants to insure the convergence for the $d$'th order derivative of the function one should choose a activation function $g \in C^n, n \geq d$. This also implies that even if the ReLu is often chosen instead of the sigmoid as activation function, one should be careful using the ReLu when approximating derivatives since convergence is not ensured. Conversely the sigmoid can be used as activation function when approximating both a function and its derivatives. The following theorem further indicates how the number of nodes and the training data set size affects the error when using the sigmoid as activation function.
\begin{thm}[Estimation bounds for Neural Networks]\label{Thm:estimation_bounds}
    Let $\mathcal{NN}_{d_0, d_1}^g$ denote the set of single layer neural networks with activation function $g(x) = \frac{\exp{(x)}}{\exp{(x)}+1}$, input dimension $d_0 \in \N$ and output dimension $d_1 \in \N$. Then
    \begin{align}\label{Eq:Estimation_bounds}
        \E{\norm{f-\hat{f}}_2^2} \leq \mathcal{O}\left(\frac{C_f^2}{n} \right) + \mathcal{O}\left(\frac{nd_0}{N} \log{(N)} \right) 
    \end{align}
    where $f: \R^{d_0} \to \R^{d_1}$ is the target function and $\hat{f}: \R^{d_0} \to \R^{d_1}$ is the function approximated by the neural network. Furthermore, $n$ denotes the number of nodes, $N$ the training data set size and $C_f$ the first absolute moment of the Fourier magnitude distribution of $f$.
\end{thm}

\begin{proof}
Omitted.
\end{proof}

Thus, the first term on the right side in \eqref{Eq:Estimation_bounds} decreases as the number of nodes increase. Though, it is also seen in the second term that the training data set size should increase as the number of nodes increases to keep the error down. This also helps prevent the neural network form overfitting. Nevertheless, \autoref{Thm:estimation_bounds} is only stated for the sigmoid, \citep[s. 117]{Barron} states that similar results can be obtained for multiple different activation functions. 

As seen in \autoref{Thm:UAT} the single layer neural network can be used to approximate any continuous function. Despite the potential effectiveness of these networks with multiple activation functions, their practicality is limited due to their exponential increase in dimension. Moreover, their high susceptibility to overfitting adds to their impracticality. When designing a neural network, a fundamental question arises regarding the optimal trade-off between its width and depth, where width and depth represents number of nodes in the layers and layers, respectively. Should one choose a narrow and deep network with multiple layers but fewer nodes per layer, or a shallow and wide network? Both empirical evidence and intuition suggest that depth plays a crucial role in the performance of a neural network. Deep architectures tend to result in complex models that are challenging to achieve with shallow networks, \citep{REOS}. In fact, Theorem 1 in \citep{REOS} states that the approximation of a function in a 2 layer neural network will require exponentially many nodes compared to a 3 layered neural network to gain the same accuracy. Thus, when designing a neural network it is preferable that it is deep instead of wide. 